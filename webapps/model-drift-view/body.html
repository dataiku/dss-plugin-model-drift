<link rel="stylesheet" href="/plugins/model-drift/resource/style.css" />
<script src="/plugins/model-drift/resource/dku-helpers.js"></script>
<script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>
<script src="https://d3js.org/d3.v4.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">


<body class="report-box">

<div class="container-fluid">
    <div class="view-content text-r">
        <div style="width: 1000px">
            Data drift analysis gives you insights on the applicability of a model by exploring how a dataset of new data differs from the evaluation (test) dataset.
            <br>
            In order to detect data drift, we train a RandomForest classifier that aims at distinguishing new data from the data present in the test set.
            If this classifier is performing well, it implies that test data and new data can be distinguished and that you are observing high data drift. You may consider retraining your model in that situation.
            <br>
            You need to provide the dataset containing new observations as input.
            <div style="margin-top: 8px">
                <a href="https://www.dataiku.com/community/plugins/info/model-drift.html" target="_blank">Plugin documentation <i class="icon-external-link"></i></a>.
            </div>
        </div>
        <div style="display: inline;">
            Dataset containing new data:
            <select id="dataset-selector" class="box text-r"></select>
            <button class="dku-btn dku-btn-primary notrunning-state" id="run-button" type="button" style="color: #FFFFFF;">COMPUTE</button>
            <span class="running-state" style="display: none;">Computing ...</span>
        </div>
    </div>
</div>

<div id="error_message" style="width: 1000px; box-sizing: border-box; padding: 0;"></div>
<div class="result-state" style="display: none; padding-top:16px; padding-bottom:16px">
    <div class="container-fluid">
        <div class="medium-title-sb" style="padding-bottom:8px">Global Drift Score</div>
        <div class="small-title-r" style="padding-bottom:8px">Drift model</div>
        <div class="row">
                <div class="col drift_score_container">
                    <div>
                        <table class="table table-condensed text-sb">
                          <tbody>
                            <tr>
                                <td colspan="2">Lower bound</td>
                                <td></td>
                                <td colspan="2">Accuracy</td>
                                <td></td>
                                <td colspan="2">Upper bound</td>
                            </tr>
                            <tr>
                                <td id="lower-bound" colspan="2" class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px">0.45</td>
                                <td class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px"><span>&#8804;</span></td>
                                <td id="accuracy" colspan="2" class="huge-title-r" style="border-top: none; vertical-align: middle; padding-top:0px">0.47</td>
                                <td class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px"><span>&#8804;</span></td>
                                <td id="upper-bound" colspan="2" class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px">0.6</td>
                            </tr>
                          </tbody>
                        </table>
                    </div>
                </div>
                <div class="col explanation text-sb"  id="drift-explanation">
                    <b>Lower is better.</b>
                    <br>
                    The drift score (between 0 and 1) is low if the test dataset (used to evaluate your model) and your input dataset are indistinguishable. See <a href="https://www.dataiku.com/community/plugins/info/model-drift.html" target="_blank">the documentation</a> for details. <br>
                    Your drift score of <span id="inline-drift-score"></span> indicates <span id="inline-drift-score-explain"></span>
                </div>
        </div>

        <div class="small-title-r" style="padding-bottom:8px">Binomial test</div>
        <div class="row">
            <div class="col">
                <table class="table table-condensed text-sb table-hover">
                  <tbody>
                    <tr>
                      <th>Hypothesis tested</th>
                      <td>There is no drift (accuracy <span>&#8804;</span> 0.5)</td>
                    </tr>
                    <tr>
                      <th>Significance level</th>
                      <td>0.05</td>
                    </tr>
                    <tr>
                      <th>p-value</th>
                      <td id="binomial-p-value">0.00020</td>
                    </tr>
                    <tr>
                      <th>Conclusion</th>
                      <td id="binomial-conclusion">< 0.05 so drift detected</td>
                    </tr>
                  </tbody>
                </table>
            </div>
            <div class="col explanation">
            Binomial description
            </div>
        </div>
    </div>
    <hr/>
    <div class="container-fluid">
        <div class="medium-title-sb" style="padding-bottom:8px">Model Information</div>
        <div class="small-title-r" id="fugacity_label" style="padding-bottom:8px">Fugacity</div>
        <div class="row" id="fugacity_div">
            <div class="col fugacity_score_container">
                <div id="fugacity-score"></div>
            </div>
            <div class="col explanation text-sb"> <b>Fugacity</b> expresses the difference between the expected "ideal" data your model was trained on and the observed "real" data you are analyzing. We compare the proportion of samples predicted in each class when scoring on both the test and your input datasets.
            </div>
        </div>

        <div class="small-title-r" id="kde_class_option" style="padding-bottom:8px padding-top:8px">Predicted probability density chart <select class="box text-r" id="label-list"></select> </div>
        <div class="row">
            <div class="col kde_chart_container" id="kde_container_div">
                <div id="kde-chart"></div>
            </div>
            <div class="col explanation text-sb" id="kde_explanation">
                This chart represents the probability density estimation for a given prediction class when scoring both the test dataset and your input dataset.
                <br><br>Visually different probability density estimations indicate high data drift.
            </div>
        </div>
    </div>
    <hr/>
    <div class="container-fluid" id="feature_importance_div">
        <div class="medium-title-sb" style="padding-bottom:8px">Feature Drift Overview</div>
        <div class="row">
            <div class="col impl_plot_container">
                <div id="feat-imp-plot"></div>
            </div>
            <div class="col explanation text-sb" >
                The scatter plot shows feature importance for the original model versus feature importance for the
                (data classifying) drift model.
                <br><br>
                <b>This graph should be examined alongside with the drift score (<span id="inline-drift-score-2"></span>)</b>.
                <br><br>
                For a highly drifted dataset (drift score ~1), if the features most responsible for data drift are of low importance
                in the original model (bottom right quadrant), you can expect the behavior of the model to remain the same.
                <br><br>
                Features in the top right quadrant of this scatter plot are highly drifted (i.e. they are powerful in
                distinguishing test data from new observations), but also of high importance for the original model.
                In this situation, you can expect the performance of the model to degrade as your model does not apply
                to your new observations.
                <br><br>
                <b>We recommend you to check the following feature(s): <span id="riskiest_features_explanation"></span></b>
            </div>
        </div>
    </div>
</div>
</body>