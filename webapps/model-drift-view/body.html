<link rel="stylesheet" href="/plugins/model-drift/resource/style.css" />
<script src="/plugins/model-drift/resource/dku-helpers.js"></script>
<script type="text/javascript" src="https://www.gstatic.com/charts/loader.js"></script>
<script src="https://d3js.org/d3.v4.js"></script>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">


<body class="report-box">

<div class="container-fluid">
    <div class="view-content text-r">
        <div style="display: inline;">
            Select dataset containing new test data
            <select id="dataset-selector" class="box2 text-r"></select>
            <button class="dku-btn dku-btn-primary notrunning-state" id="run-button" type="button" style="color: #FFFFFF;">COMPUTE DRIFT</button>
            <span class="running-state" style="display: none;">Computing ...</span>
        </div>
    </div>
</div>

<div class="landing-page" style="padding-top:40px; padding-bottom:40px; padding-left:100px; padding-right:100px">
    </br> </br> </br></br></br>
<div class="ultralarge-title-sb" style="padding-bottom:16px">
    Input Data Drift
</div>
<div class="grand-title-landing">
    Get insights on the applicability of a model by exploring how a dataset of new data differs from the original evaluation (test) dataset.
</div>
</div>

<div class="container-fluid" id="error_message" style="box-sizing: border-box; padding: 0;"></div>
<div class="result-state" style="display: none; padding-top:16px; padding-bottom:16px">
    <div class="container-fluid">
        <div class="medium-title-sb" style="padding-bottom:4px">Global Drift Score</div>
        <div class="tiny-text-r" style="padding-bottom:16px">Sampling first <b><span id="sample-size"></span></b> rows per dataset</div>
        <div class="small-title-r" style="padding-bottom:16px">Drift model</div>
        <div class="row">
                <div class="col drift_score_container">
                    <div>
                        <table class="table table-condensed text-sb">
                          <tbody>
                            <tr>
                                <td colspan="2">Lower bound</td>
                                <td></td>
                                <td colspan="2">Accuracy</td>
                                <td></td>
                                <td colspan="2">Upper bound</td>
                            </tr>
                            <tr>
                                <td id="lower-bound" colspan="2" class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px">0.45</td>
                                <td class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px"><span>&#8804;</span></td>
                                <td id="accuracy" colspan="2" class="huge-title-r" style="border-top: none; vertical-align: middle; padding-top:0px">0.47</td>
                                <td class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px"><span>&#8804;</span></td>
                                <td id="upper-bound" colspan="2" class="grand-title-r" style="border-top: none; vertical-align: middle; padding-top:0px">0.6</td>
                            </tr>
                          </tbody>
                        </table>
                    </div>
                </div>
                <div class="col explanation text-r"  id="drift-explanation">
                    <b>Lower is better.</b>
                    <br>
                    In order to detect data drift, we train a random forest classifier (the drift model) to discriminate the new data set from the test set. If this classifier has accuracy > 0.5, it implies that test data and new data can be distinguished and that you are observing data drift. You may consider retraining your model in that situation.
                </div>
        </div>

        <div class="small-title-r" style="padding-bottom:16px">Binomial test</div>
        <div class="row">
            <div class="col">
                <table class="table table-condensed text-sb table-hover">
                  <tbody>
                    <tr>
                      <th>Hypothesis tested</th>
                      <td>There is no drift (accuracy <span>&#8804;</span> 0.5)</td>
                    </tr>
                    <tr>
                      <th>Significance level</th>
                      <td>0.05</td>
                    </tr>
                    <tr>
                      <th>p-value</th>
                      <td id="binomial-p-value">0.00020</td>
                    </tr>
                    <tr>
                      <th>Conclusion</th>
                      <td id="binomial-conclusion"><span>&#8804;</span> 0.05 so drift detected</td>
                    </tr>
                  </tbody>
                </table>
            </div>
            <div class="col explanation text-r">
                <b>The hypothesis tested</b> is that there is no drift, in which case the expected drift model accuracy is 0.5 (datasets undistinguishable). The observed accuracy might deviate from this expectation and the Binomial test evaluates whether this deviation is statistically significant, modelling the number of correct predictions as a random variable drawn from a Binomial distribution.
                <br>
                The p-value is the probability to observe this particular accuracy (or larger) under the hypothesis of absent drift. If this probability is lower than the significance level (i.e. 5%), itâ€™s then unlikely to be in the situation of absent drift: the hypothesis of no drift is rejected, triggering a drift detection. The significance level indicates the rate of falsely-detected drifts we are ready to accept from the test.
            </div>
        </div>
    </div>
    <hr/>
    <div class="container-fluid">
        <div class="medium-title-sb" style="padding-bottom:16px">Model Information</div>
        <div class="small-title-r" id="fugacity_label" style="padding-bottom:16px">Fugacity</div>
        <div class="row" id="fugacity_div">
            <div class="col fugacity_score_container">
                <div id="fugacity-score"></div>
            </div>
            <div class="col explanation text-r"> <b>Fugacity</b> expresses the difference between the expected "ideal" data your model was trained on and the observed "real" data you are analyzing. We compare the proportion of samples predicted in each class when scoring on both the test and your input datasets.
            </div>
        </div>

        <div class="small-title-r" id="kde_class_option" style="padding-bottom:16px padding-top:16px">Predicted probability density chart <select class="box text-r" id="label-list"></select> </div>
        <div class="row">
            <div class="col kde_chart_container" id="kde_container_div">
                <div id="kde-chart"></div>
            </div>
            <div class="col explanation text-r" id="kde_explanation">
                This chart represents the probability density estimation for a given prediction class when scoring both the test dataset and your input dataset.
                <br><br>Visually different probability density estimations indicate high data drift.
            </div>
        </div>
    </div>
    <hr/>
    <div class="container-fluid" id="feature_importance_div">
        <div class="medium-title-sb" style="padding-bottom:16px">Feature Drift Overview</div>
        <div class="row">
            <div class="col impl_plot_container">
                <div id="feat-imp-plot"></div>
            </div>
            <div class="col explanation text-r" >
                The scatter plot shows feature importance for the original model versus feature importance for the
                (data classifying) drift model.
                <br><br>
                <b>This graph should be examined alongside with the drift score (<span id="inline-drift-score-2"></span>)</b>.
                <br><br>
                For a highly drifted dataset (drift score ~1), if the features most responsible for data drift are of low importance
                in the original model (bottom right quadrant), you can expect the behavior of the model to remain the same.
                <br><br>
                Features in the top right quadrant of this scatter plot are highly drifted (i.e. they are powerful in
                distinguishing test data from new observations), but also of high importance for the original model.
                In this situation, you can expect the performance of the model to degrade as your model does not apply
                to your new observations.
                <br><br>
                <b><span id="riskiest_features_explanation"></span></b>
            </div>
        </div>
    </div>
</div>
</body>